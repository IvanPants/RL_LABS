{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPGPfi8EajGp"
      },
      "source": [
        "Реализуйте алгоритм GAIL на среде Mountain Car. Перед этим сгенерируйте экспертные данные (из детерминированной стратегии с первой практики). Хорошей идеей будет добавить в state (observation) синус и косинус от временной метки t для лучшего обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L1-l3BqFbaUV"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jsj3u8hwYAVe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5o6iKkn5rCh0"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rPY0rlr3dhIf"
      },
      "outputs": [],
      "source": [
        "def generate_expert_data(env, num_episodes=100):\n",
        "    states = []\n",
        "    actions = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        t = 0  # Временная метка\n",
        "\n",
        "        while not done:\n",
        "            # Выбираем действие с помощью экспертной стратегии\n",
        "            action = expert_policy(obs)\n",
        "\n",
        "            # Совершаем шаг в среде\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Дополняем состояние синусом и косинусом временной метки t\n",
        "            extended_state = np.append(obs, [np.sin(t), np.cos(t)])\n",
        "            states.append(extended_state)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Обновляем состояние и временную метку\n",
        "            obs = next_obs\n",
        "            t += 1\n",
        "\n",
        "    return np.array(states, dtype=np.float32), np.array(actions, dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0XKtvaQBdiwx"
      },
      "outputs": [],
      "source": [
        "def expert_policy(obs):\n",
        "    _, velocity = obs\n",
        "    if velocity < 0:\n",
        "        return 0  # Толкать влево\n",
        "    else:\n",
        "        return 2  # Толкать вправо"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWlezxzcPzbT",
        "outputId": "eae02648-32f1-4cda-bd4f-8532502f3928"
      },
      "outputs": [],
      "source": [
        "states, actions = generate_expert_data(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyLq9c-2bvXE",
        "outputId": "7b6580c7-190b-46e6-ce83-e6bf795d4491"
      },
      "outputs": [],
      "source": [
        "obs_dim = 4\n",
        "act_dim = 3\n",
        "expert_obs, expert_acts = generate_expert_data(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wp56QsLpcT0M"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, act_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        logits = self.net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        dist = self.forward(obs)\n",
        "        return dist.sample().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PYe2ekNUcUf5"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, obs, act):\n",
        "        act_onehot = F.one_hot(act, num_classes=3).float()\n",
        "        x = torch.cat([obs, act_onehot], dim=1)\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cV5NtwYMccNS"
      },
      "outputs": [],
      "source": [
        "class TrajectoryBuffer:\n",
        "    def __init__(self):\n",
        "        self.obs, self.acts, self.rews = [], [], []\n",
        "\n",
        "    def store(self, o, a, r):\n",
        "        self.obs.append(o)\n",
        "        self.acts.append(a)\n",
        "        self.rews.append(r)\n",
        "\n",
        "    def get(self):\n",
        "        return (\n",
        "            torch.tensor(np.array(self.obs), dtype=torch.float32),\n",
        "            torch.tensor(np.array(self.acts), dtype=torch.long),\n",
        "            torch.tensor(np.array(self.rews), dtype=torch.float32)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MxfjF-ZjceE7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "policy = Policy(obs_dim, act_dim)\n",
        "discrim = Discriminator(obs_dim, act_dim)\n",
        "\n",
        "policy_opt = optim.Adam(policy.parameters(), lr=3e-4)\n",
        "discrim_opt = optim.Adam(discrim.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8YJdh50afhGn"
      },
      "outputs": [],
      "source": [
        "\n",
        "expert_obs_tensor = torch.tensor(expert_obs, dtype=torch.float32)\n",
        "expert_acts_tensor = torch.tensor(expert_acts, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXvmzCYxiXCg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "R5rKCyC0ch0W",
        "outputId": "2efe80f2-7659-41a1-a0dc-161c00510e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Policy Loss 0.76, Disc Loss 0.70\n",
            "Epoch 10: Policy Loss 0.76, Disc Loss 0.68\n",
            "Epoch 20: Policy Loss 0.79, Disc Loss 0.67\n",
            "Epoch 30: Policy Loss 0.80, Disc Loss 0.65\n",
            "Epoch 40: Policy Loss 0.83, Disc Loss 0.63\n",
            "Epoch 50: Policy Loss 0.88, Disc Loss 0.60\n",
            "Epoch 60: Policy Loss 0.95, Disc Loss 0.59\n",
            "Epoch 70: Policy Loss 1.04, Disc Loss 0.55\n",
            "Epoch 80: Policy Loss 1.11, Disc Loss 0.53\n",
            "Epoch 90: Policy Loss 1.23, Disc Loss 0.48\n",
            "Epoch 100: Policy Loss 1.28, Disc Loss 0.46\n",
            "Epoch 110: Policy Loss 1.36, Disc Loss 0.43\n",
            "Epoch 120: Policy Loss 1.34, Disc Loss 0.39\n",
            "Epoch 130: Policy Loss 1.24, Disc Loss 0.40\n",
            "Epoch 140: Policy Loss 1.28, Disc Loss 0.33\n",
            "Epoch 150: Policy Loss 1.24, Disc Loss 0.30\n",
            "Epoch 160: Policy Loss 1.16, Disc Loss 0.26\n",
            "Epoch 170: Policy Loss 1.03, Disc Loss 0.29\n",
            "Epoch 180: Policy Loss 0.98, Disc Loss 0.21\n",
            "Epoch 190: Policy Loss 0.87, Disc Loss 0.21\n",
            "Epoch 200: Policy Loss 0.78, Disc Loss 0.20\n",
            "Epoch 210: Policy Loss 0.69, Disc Loss 0.18\n",
            "Epoch 220: Policy Loss 0.62, Disc Loss 0.15\n",
            "Epoch 230: Policy Loss 0.55, Disc Loss 0.13\n",
            "Epoch 240: Policy Loss 0.51, Disc Loss 0.14\n",
            "Epoch 250: Policy Loss 0.44, Disc Loss 0.13\n",
            "Epoch 260: Policy Loss 0.40, Disc Loss 0.11\n",
            "Epoch 270: Policy Loss 0.38, Disc Loss 0.15\n",
            "Epoch 280: Policy Loss 0.33, Disc Loss 0.10\n",
            "Epoch 290: Policy Loss 0.30, Disc Loss 0.08\n",
            "Epoch 300: Policy Loss 0.28, Disc Loss 0.06\n",
            "Epoch 310: Policy Loss 0.25, Disc Loss 0.07\n",
            "Epoch 320: Policy Loss 0.23, Disc Loss 0.06\n",
            "Epoch 330: Policy Loss 0.21, Disc Loss 0.05\n",
            "Epoch 340: Policy Loss 0.20, Disc Loss 0.05\n",
            "Epoch 350: Policy Loss 0.19, Disc Loss 0.03\n",
            "Epoch 360: Policy Loss 0.17, Disc Loss 0.07\n",
            "Epoch 370: Policy Loss 0.16, Disc Loss 0.04\n",
            "Epoch 380: Policy Loss 0.15, Disc Loss 0.04\n",
            "Epoch 390: Policy Loss 0.14, Disc Loss 0.06\n",
            "Epoch 400: Policy Loss 0.13, Disc Loss 0.05\n",
            "Epoch 410: Policy Loss 0.12, Disc Loss 0.05\n",
            "Epoch 420: Policy Loss 0.12, Disc Loss 0.03\n",
            "Epoch 430: Policy Loss 0.11, Disc Loss 0.06\n",
            "Epoch 440: Policy Loss 0.10, Disc Loss 0.04\n",
            "Epoch 450: Policy Loss 0.10, Disc Loss 0.02\n",
            "Epoch 460: Policy Loss 0.09, Disc Loss 0.04\n",
            "Epoch 470: Policy Loss 0.09, Disc Loss 0.01\n",
            "Epoch 480: Policy Loss 0.08, Disc Loss 0.01\n",
            "Epoch 490: Policy Loss 0.08, Disc Loss 0.04\n",
            "Epoch 500: Policy Loss 0.08, Disc Loss 0.02\n",
            "Epoch 510: Policy Loss 0.08, Disc Loss 0.03\n",
            "Epoch 520: Policy Loss 0.07, Disc Loss 0.02\n",
            "Epoch 530: Policy Loss 0.07, Disc Loss 0.02\n",
            "Epoch 540: Policy Loss 0.06, Disc Loss 0.05\n",
            "Epoch 550: Policy Loss 0.06, Disc Loss 0.02\n",
            "Epoch 560: Policy Loss 0.06, Disc Loss 0.01\n",
            "Epoch 570: Policy Loss 0.05, Disc Loss 0.01\n",
            "Epoch 580: Policy Loss 0.05, Disc Loss 0.03\n",
            "Epoch 590: Policy Loss 0.05, Disc Loss 0.00\n",
            "Epoch 600: Policy Loss 0.05, Disc Loss 0.01\n",
            "Epoch 610: Policy Loss 0.05, Disc Loss 0.00\n",
            "Epoch 620: Policy Loss 0.05, Disc Loss 0.00\n",
            "Epoch 630: Policy Loss 0.04, Disc Loss 0.04\n",
            "Epoch 640: Policy Loss 0.04, Disc Loss 0.03\n",
            "Epoch 650: Policy Loss 0.04, Disc Loss 0.03\n",
            "Epoch 660: Policy Loss 0.04, Disc Loss 0.00\n",
            "Epoch 670: Policy Loss 0.04, Disc Loss 0.01\n",
            "Epoch 680: Policy Loss 0.04, Disc Loss 0.02\n",
            "Epoch 690: Policy Loss 0.03, Disc Loss 0.01\n",
            "Epoch 700: Policy Loss 0.03, Disc Loss 0.02\n",
            "Epoch 710: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 720: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 730: Policy Loss 0.03, Disc Loss 0.01\n",
            "Epoch 740: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 750: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 760: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 770: Policy Loss 0.03, Disc Loss 0.01\n",
            "Epoch 780: Policy Loss 0.03, Disc Loss 0.00\n",
            "Epoch 790: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 800: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 810: Policy Loss 0.02, Disc Loss 0.04\n",
            "Epoch 820: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 830: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 840: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 850: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 860: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 870: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 880: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 890: Policy Loss 0.02, Disc Loss 0.03\n",
            "Epoch 900: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 910: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 920: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 930: Policy Loss 0.02, Disc Loss 0.02\n",
            "Epoch 940: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 950: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 960: Policy Loss 0.02, Disc Loss 0.00\n",
            "Epoch 970: Policy Loss 0.02, Disc Loss 0.02\n",
            "Epoch 980: Policy Loss 0.01, Disc Loss 0.00\n",
            "Epoch 990: Policy Loss 0.01, Disc Loss 0.00\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1000):\n",
        "    buf = TrajectoryBuffer()\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "\n",
        "    while not done: \n",
        "        augmented_obs = np.append(obs, [np.sin(t), np.cos(t)])\n",
        "        obs_tensor = torch.tensor(augmented_obs, dtype=torch.float32).unsqueeze(0)\n",
        "        action = policy.get_action(obs_tensor)\n",
        "\n",
        "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        buf.store(augmented_obs, action, 0)\n",
        "        obs = next_obs\n",
        "        t += 1\n",
        "\n",
        "    agent_obs, agent_acts, _ = buf.get()\n",
        "\n",
        "    for _ in range(5):\n",
        "        discrim_opt.zero_grad()\n",
        "        \n",
        "        # Выбираем случайные экспертные данные\n",
        "        expert_idx = np.random.choice(len(expert_obs_tensor), len(agent_obs), replace=False)\n",
        "        expert_batch_obs = expert_obs_tensor[expert_idx]\n",
        "        expert_batch_acts = expert_acts_tensor[expert_idx]\n",
        "        \n",
        "        expert_probs = discrim(expert_batch_obs, expert_batch_acts)\n",
        "        agent_probs = discrim(agent_obs, agent_acts)\n",
        "        \n",
        "        disc_loss = (\n",
        "            F.binary_cross_entropy(expert_probs, torch.ones_like(expert_probs)) +\n",
        "            F.binary_cross_entropy(agent_probs, torch.zeros_like(agent_probs))\n",
        "        ) / 2\n",
        "        \n",
        "        disc_loss.backward()\n",
        "        discrim_opt.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        agent_probs = discrim(agent_obs, agent_acts)\n",
        "        rewards = -torch.log(agent_probs + 1e-8)\n",
        "\n",
        "    # Обучение политики\n",
        "    policy_opt.zero_grad()\n",
        "    dists = policy(agent_obs)\n",
        "    log_probs = dists.log_prob(agent_acts)\n",
        "    loss = -(log_probs * rewards.squeeze()).mean()\n",
        "    loss.backward()\n",
        "    policy_opt.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Policy Loss {loss.item():.2f}, Disc Loss {disc_loss.item():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu1Cjaikdwg7"
      },
      "source": [
        "Протестируйте ваш алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm4v4361UcSS",
        "outputId": "a4795a39-400c-45aa-fb84-0d2cf7661768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1: Total Reward = -200.0\n",
            "Test Episode 2: Total Reward = -200.0\n",
            "Test Episode 3: Total Reward = -200.0\n",
            "Test Episode 4: Total Reward = -200.0\n",
            "Test Episode 5: Total Reward = -200.0\n",
            "Test Episode 6: Total Reward = -200.0\n",
            "Test Episode 7: Total Reward = -200.0\n",
            "Test Episode 8: Total Reward = -200.0\n",
            "Test Episode 9: Total Reward = -200.0\n",
            "Test Episode 10: Total Reward = -200.0\n",
            "Test Episode 11: Total Reward = -200.0\n",
            "Test Episode 12: Total Reward = -200.0\n",
            "Test Episode 13: Total Reward = -200.0\n",
            "Test Episode 14: Total Reward = -200.0\n",
            "Test Episode 15: Total Reward = -200.0\n",
            "Test Episode 16: Total Reward = -200.0\n",
            "Test Episode 17: Total Reward = -200.0\n",
            "Test Episode 18: Total Reward = -200.0\n",
            "Test Episode 19: Total Reward = -200.0\n",
            "Test Episode 20: Total Reward = -200.0\n",
            "highest point -0.4923360049724579\n"
          ]
        }
      ],
      "source": [
        "for episode in range(20):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    t = 0\n",
        "    obses = []\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            augmented_obs = np.append(obs, [np.sin(t), np.cos(t)])\n",
        "            obs_tensor = torch.tensor(augmented_obs, dtype=torch.float32).unsqueeze(0)\n",
        "            action = policy.get_action(obs_tensor)\n",
        "\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            obses.append(obs[0])\n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            t += 1\n",
        "        \n",
        "    print(f\"Test Episode {episode+1}: Total Reward = {total_reward}\")\n",
        "print(f\"highest point {max(obses)}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdPR2rX_ttAV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
